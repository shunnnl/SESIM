import pandas as pd, hashlib, json, logging
from pathlib import Path
from tempfile import TemporaryDirectory

from fastapi import APIRouter, HTTPException, UploadFile, File
from app.dto.response import TrainResponse
from app.services.trainer import robust_incremental_training, _read_history
from app.core.config import MODEL_DIR, TRAINING_HISTORY_DIR
from app.core.registry import (
    get_next_model_version,
    get_available_model_versions,
    clear_bundle_cache,
    create_initial_model_structure,
    has_trained_model,
)

router = APIRouter(tags=["í•™ìŠµ API"])
logger = logging.getLogger(__name__)

def is_duplicate_file(csv: Path) -> bool:
    """íŒŒì¼ í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬"""
    logger.info("=== ì¤‘ë³µ íŒŒì¼ ì²´í¬ ì‹œì‘ ===")
    
    TRAINING_HISTORY_DIR.mkdir(parents=True, exist_ok=True)
    fh = TRAINING_HISTORY_DIR / "file_hashes.json"
    
    # í˜„ì¬ íŒŒì¼ í•´ì‹œ ê³„ì‚°
    current_hash = hashlib.sha256(csv.read_bytes()).hexdigest()
    logger.info(f"í˜„ì¬ íŒŒì¼ í•´ì‹œ: {current_hash}")
    logger.info(f"í•´ì‹œ íŒŒì¼ ê²½ë¡œ: {fh}")
    
    # ê¸°ì¡´ í•´ì‹œ ëª©ë¡ ë¡œë“œ
    if fh.exists():
        try:
            hashes = json.loads(fh.read_text())
            logger.info(f"ê¸°ì¡´ í•´ì‹œ ëª©ë¡: {hashes}")
        except Exception as e:
            logger.error(f"í•´ì‹œ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            hashes = []
    else:
        hashes = []
        logger.info("í•´ì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
    
    # ì¤‘ë³µ ì²´í¬
    if current_hash in hashes:
        logger.info("ğŸ”„ ë™ì¼í•œ íŒŒì¼ì´ ì´ì „ì— í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤!")
        return True
    
    # ìƒˆ í•´ì‹œ ì¶”ê°€
    hashes.append(current_hash)
    try:
        fh.write_text(json.dumps(hashes, indent=2))
        logger.info(f"âœ… ìƒˆ íŒŒì¼ í•´ì‹œë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤. ì´ {len(hashes)}ê°œ")
    except Exception as e:
        logger.error(f"í•´ì‹œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    logger.info("=== ì¤‘ë³µ íŒŒì¼ ì²´í¬ ì™„ë£Œ ===")
    return False

@router.post("/train", response_model=TrainResponse)
async def train_endpoint(file: UploadFile = File(...)):
    logger.info("ğŸš€ === í•™ìŠµ ì—”ë“œí¬ì¸íŠ¸ ì‹œì‘ ===")
    
    if not file.filename.endswith(".csv"):
        raise HTTPException(status_code=400, detail="CSV íŒŒì¼ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

    # ì´ˆê¸° ëª¨ë¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
    create_initial_model_structure()

    with TemporaryDirectory() as tmp:
        tmp_csv = Path(tmp) / file.filename
        tmp_csv.write_bytes(await file.read())
        
        logger.info(f"ğŸ“ ì—…ë¡œë“œëœ íŒŒì¼: {file.filename}")
        logger.info(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {tmp_csv.stat().st_size} bytes")

        # ëª¨ë¸ ìƒíƒœ í™•ì¸
        available_versions = get_available_model_versions()
        has_model = has_trained_model()
        
        logger.info(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë²„ì „ë“¤: {available_versions}")
        logger.info(f"ğŸ¯ í•™ìŠµëœ ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€: {has_model}")
        logger.info(f"ğŸ“‚ MODEL_DIR ë‚´ìš©: {list(MODEL_DIR.glob('*')) if MODEL_DIR.exists() else 'ë””ë ‰í† ë¦¬ ì—†ìŒ'}")

        # ì²« ë²ˆì§¸ í•™ìŠµì¸ì§€ í™•ì¸
        is_first_training = not has_model
        logger.info(f"ğŸ†• ì²« ë²ˆì§¸ í•™ìŠµ ì—¬ë¶€: {is_first_training}")
        
        # â­ï¸ ì¤‘ë³µ ì²´í¬ (ì²« ë²ˆì§¸ í•™ìŠµì´ë“  ì•„ë‹ˆë“  í•­ìƒ ì²´í¬)
        logger.info("ğŸ”„ ì¤‘ë³µ ì²´í¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        if is_duplicate_file(tmp_csv):
            latest = available_versions[0] if available_versions else "1.0.0"
            logger.info(f"â­ï¸  íŒŒì¼ì´ ì¤‘ë³µë˜ì–´ í•™ìŠµì„ ìŠ¤í‚µí•©ë‹ˆë‹¤. ìµœì‹  ë²„ì „: {latest}")
            return TrainResponse(
                message="íŒŒì¼ ì „ì²´ê°€ ì´ì „ê³¼ ë™ì¼ â†’ ìŠ¤í‚µ", 
                version=latest
            )
        
        logger.info("âœ… ìƒˆë¡œìš´ íŒŒì¼ì…ë‹ˆë‹¤. í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.")

        try:
            new_df = pd.read_csv(tmp_csv)
            logger.info(f"ğŸ“‹ CSV ë¡œë“œ ì™„ë£Œ: {len(new_df)}í–‰, {len(new_df.columns)}ì»¬ëŸ¼")
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_columns = ["url", "method", "user_agent", "status_code", "is_attack"]
            missing_columns = [col for col in required_columns if col not in new_df.columns]
            if missing_columns:
                raise HTTPException(
                    status_code=400, 
                    detail=f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_columns}"
                )
            
            # is_attackì´ nullì¸ í–‰ ì²˜ë¦¬
            if "is_attack" in new_df.columns and new_df["is_attack"].isnull().any():
                null_count = new_df["is_attack"].isnull().sum()
                new_df["is_attack"] = new_df["is_attack"].fillna(False)
                new_df.to_csv(tmp_csv, index=False)
                logger.info(f"ğŸ”§ is_attack null ê°’ {null_count}ê°œë¥¼ Falseë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
                
            # ë°ì´í„° ì‚¬ìš© (ì²« ë²ˆì§¸ë“  ì•„ë‹ˆë“  ì „ì²´ ë°ì´í„° ì‚¬ìš©)
            novel_df = new_df
            
            if is_first_training:
                logger.info("ğŸŒŸ ì²« ë²ˆì§¸ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            else:
                logger.info("ğŸ”„ ì¶”ê°€ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")

            # ë°ì´í„° ê²€ì¦
            if len(novel_df) < 10:
                raise HTTPException(
                    status_code=400,
                    detail=f"í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìµœì†Œ 10ê°œ ì´ìƒì˜ í–‰ì´ í•„ìš”í•©ë‹ˆë‹¤. (í˜„ì¬: {len(novel_df)}ê°œ)"
                )
            
            # ê³µê²© ë°ì´í„° ë¹„ìœ¨ í™•ì¸
            attack_ratio = novel_df["is_attack"].mean()
            logger.info(f"âš”ï¸  ê³µê²© ë°ì´í„° ë¹„ìœ¨: {attack_ratio:.2%}")
            if attack_ratio == 0:
                logger.warning("âš ï¸  ê³µê²© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ì§„ ë¶„ë¥˜ë§Œ í•™ìŠµë©ë‹ˆë‹¤.")
            elif attack_ratio < 0.01:
                logger.warning(f"âš ï¸  ê³µê²© ë°ì´í„° ë¹„ìœ¨ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤: {attack_ratio:.2%}")

            nov_csv = Path(tmp) / "novel.csv"
            novel_df.to_csv(nov_csv, index=False)

            next_ver = get_next_model_version()
            logger.info(f"ğŸ·ï¸  ë‹¤ìŒ ëª¨ë¸ ë²„ì „: {next_ver}")
            logger.info(f"ğŸ“ ëª¨ë¸ í•™ìŠµ ì‹œì‘ - ë°ì´í„°: {len(novel_df)}í–‰")

            # í†µí•© ëª¨ë¸ í•™ìŠµ
            result = robust_incremental_training(csv_path=nov_csv, model_version=next_ver)
            logger.info(f"âœ… í•™ìŠµ ì™„ë£Œ: {result}")

            # ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™”
            clear_bundle_cache()
            
            success_message = "ì²« ë²ˆì§¸ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ" if is_first_training else "ëª¨ë¸ í•™ìŠµ ì™„ë£Œ"
            logger.info(f"ğŸ‰ {success_message}")
            
            return TrainResponse(
                message=success_message, 
                version=next_ver
            )
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"í•™ìŠµ ì‹¤íŒ¨: {str(e)}")