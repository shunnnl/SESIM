from __future__ import annotations

import re 
import json
import time
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple

import joblib
import numpy as np
import pandas as pd
from scipy import sparse
from xgboost import XGBClassifier
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from sklearn.feature_extraction.text import TfidfVectorizer

from app.core.config import (
    BIN_PARAMS,
    TYPE_PARAMS,
    MODEL_DIR,
    RNG,
    VEC_MAX_FEAT,
    EXPECTED_META_FEATURE_DIMS,
    SUPPORTED_ATTACK_TYPES,
    normalize_attack_type,
    get_pattern_feature_dims,
    TRAINING_OPTIMIZATION_LEVEL,
)
from app.core.encoder import UnifiedEncoder
from app.utils import (
    PATTERN_COLS,
    extract_url_features,
    preprocess_url,
    build_meta_features,
    validate_meta_features,
)

logger = logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ëª¨ë¸ ë²ˆë“¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class UnifiedModelBundle:

    def __init__(self) -> None:
        self.vectorizer = None
        self.encoder = UnifiedEncoder()
        self.binary_classifier = None
        self.type_classifier = None
        self.version = None
        self.meta: Dict = {}

    # ì €ìž¥Â·ë¡œë“œ
    def save(self, model_dir: Path) -> None:
        model_dir.mkdir(parents=True, exist_ok=True)
        joblib.dump(self.vectorizer, model_dir / "vectorizer.pkl", compress=3)
        joblib.dump(self.encoder,    model_dir / "unified_encoder.pkl", compress=3)
        joblib.dump(
            {
                "binary_classifier": self.binary_classifier,
                "type_classifier":   self.type_classifier,
            },
            model_dir / "models.pkl",
            compress=3,
        )
        (model_dir / "meta.json").write_text(json.dumps(self.meta, indent=2))
        logger.info("í†µí•© ëª¨ë¸ ë²ˆë“¤ ì €ìž¥: %s", model_dir)

    @staticmethod
    def load(model_dir: Path) -> "UnifiedModelBundle":
        bundle = UnifiedModelBundle()
        bundle.vectorizer = joblib.load(model_dir / "vectorizer.pkl")
        bundle.encoder    = joblib.load(model_dir / "unified_encoder.pkl")
        models = joblib.load(model_dir / "models.pkl")
        bundle.binary_classifier = models["binary_classifier"]
        bundle.type_classifier   = models.get("type_classifier")
        meta_path = model_dir / "meta.json"
        if meta_path.exists():
            bundle.meta = json.loads(meta_path.read_text())
        return bundle


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ë°ì´í„° ì¤€ë¹„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def prepare_training_data(csv_path: Path) -> Tuple[sparse.csr_matrix, np.ndarray, UnifiedModelBundle]:

    df = pd.read_csv(csv_path)

    if "attack_type" in df.columns:
        df = df[
            df["is_attack"].eq(False)
            | (
                df["attack_type"].notna()
                & df["attack_type"].astype(str).str.strip().ne("")
            )
        ]

    df["is_attack"] = df["is_attack"].fillna(False)
    df["url_prep"]  = df["url"].apply(preprocess_url)

    bundle = UnifiedModelBundle()

    # 2-1. URL TF-IDF
    bundle.vectorizer = TfidfVectorizer(
        analyzer="char_wb",
        ngram_range=(3, 5),
        max_features=VEC_MAX_FEAT,
        min_df=3,
        dtype=np.float32,
    )
    X_txt = bundle.vectorizer.fit_transform(df["url_prep"])

    # 2-2. ë©”ì„œë“œÂ·UA ì¸ì½”ë”©
    bundle.encoder.fit_method_agent(
        df["method"].fillna("GET").tolist(),
        df["user_agent"].fillna("").tolist(),
    )

    # 2-3. ë©”íƒ€ í”¼ì²˜
    X_meta = build_meta_features(df, bundle.encoder)
    validate_meta_features(X_meta, EXPECTED_META_FEATURE_DIMS)

    # 2-4. íŒ¨í„´ í”¼ì²˜
    X_ptrn = sparse.csr_matrix(
        extract_url_features(df)[PATTERN_COLS].values.astype(np.float32)
    )
    exp_dims = get_pattern_feature_dims()
    if X_ptrn.shape[1] != exp_dims:
        raise ValueError(f"íŒ¨í„´ í”¼ì²˜ ì°¨ì› ë¶ˆì¼ì¹˜: ì˜ˆìƒ {exp_dims}, ì‹¤ì œ {X_ptrn.shape[1]}")

    # 2-5. ê²°í•©
    X = sparse.hstack([X_txt, X_meta, X_ptrn], format="csr")
    y = df["is_attack"].astype(np.int8).to_numpy()

    return X, y, bundle


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ì›¹ì…¸ ìš°ì„  ìžë™ ë¼ë²¨ë§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def auto_label_attacks(df: pd.DataFrame) -> pd.DataFrame:

    webshell_patterns = [
        r"(\\?|&)(cmd|command|exec|shell|system|run|action|do|op|operation)=",
        r"\.(jsp|php)\?.*(?:cmd|command|exec|shell)=",
        r"/shells?/.*\.(jsp|php)",
        r"(c99|r57|wso|b374k|webshell|backdoor)\.(php|jsp)",
    ]
    injection_patterns = [
        r"\b(cmd|exec|system)\b",
        r"(bash|sh)\s+-c",
        r"\b(cat|ls|nc|curl|wget)\b\s",
    ]
    dt_patterns = [
        r"\.\./\.\./",
        r"\b/etc/passwd\b",
        r"\bboot.ini\b",
    ]
    xss_patterns = [
        r"(?i)<script\b[^>]*>",
        r"(?i)javascript:",
        r"alert\s*\(",
    ]
    sqli_patterns = [
        r"(?i)\bunion\b.*\bselect\b",
        r"(?i)(--|#|/\*)",
        r"(\bor\b.+(?:=|like).*?\btrue\b|\b1[ =]1\b)",
    ]
    ssrf_patterns = [
        r"\b(?:file|dict|gopher|http|https|tcp|ftp):\/\/",
        r"@169\.254\.169\.254",
        r"metadata\.google\.internal",
    ]

    def hit(text: str, patterns: List[str]) -> bool:
        return bool(text) and any(re.search(p, text, re.I) for p in patterns)

    for idx, row in df.iterrows():
        if row.get("attack_type"):
            continue
        combined = f"{row.get('url','')} {row.get('user_agent','')}".lower()
        if hit(combined, webshell_patterns):
            df.at[idx, "attack_type"] = "webshell"
        elif hit(combined, injection_patterns):
            df.at[idx, "attack_type"] = "command_injection"
        elif hit(combined, dt_patterns):
            df.at[idx, "attack_type"] = "directory_traversal"
        elif hit(combined, xss_patterns):
            df.at[idx, "attack_type"] = "xss"
        elif hit(combined, sqli_patterns):
            df.at[idx, "attack_type"] = "sql_injection"
        elif hit(combined, ssrf_patterns):
            df.at[idx, "attack_type"] = "ssrf_rfi"
    return df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. íŠ¸ë ˆì´ë„ˆ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class UnifiedTrainer:

    def __init__(self, version: str) -> None:
        self.version = str(version)
        self.out_dir = MODEL_DIR / f"model_v{self.version}"
        if self.out_dir.exists():
            raise FileExistsError(f"{self.out_dir} ì´ë¯¸ ì¡´ìž¬í•©ë‹ˆë‹¤")

    def train(self, csv: str | Path) -> Dict[str, str]:
        csv = Path(csv)
        t0 = time.time()
        logger.info("ðŸš€ í•™ìŠµ ì‹œìž‘ (opt-level: %s)", TRAINING_OPTIMIZATION_LEVEL)

        # 4-1. ë°ì´í„° ì¤€ë¹„
        X, y, bundle = prepare_training_data(csv)

        # 4-2. ì´ì§„ ë¶„ë¥˜
        if len(y) > 100_000:
            sampler = RandomUnderSampler(random_state=RNG, sampling_strategy=0.8)
            X_bin, y_bin = sampler.fit_resample(X, y)
        else:
            X_bin, y_bin = X, y
        bundle.binary_classifier = XGBClassifier(**BIN_PARAMS).fit(X_bin, y_bin)

        # 4-3. ë‹¤ì¤‘ ë¶„ë¥˜
        df_full = pd.read_csv(csv)
        df_full = auto_label_attacks(df_full)
        attack_df = df_full[df_full["is_attack"]]
        if not attack_df.empty and "attack_type" in attack_df.columns:
            attack_df = attack_df.dropna(subset=["attack_type"])
            attack_df["attack_type"] = attack_df["attack_type"].apply(normalize_attack_type)
            attack_df = attack_df[attack_df["attack_type"].isin(SUPPORTED_ATTACK_TYPES)]

            bundle.encoder.fit_attack_types(attack_df["attack_type"].tolist())
            y_type = bundle.encoder.transform_attack_types(attack_df["attack_type"].tolist())

            X_txt = bundle.vectorizer.transform(attack_df["url"].apply(preprocess_url))
            X_meta = build_meta_features(attack_df, bundle.encoder)
            validate_meta_features(X_meta, EXPECTED_META_FEATURE_DIMS)
            X_ptrn = sparse.csr_matrix(
                extract_url_features(attack_df)[PATTERN_COLS].values.astype(np.float32)
            )
            X_type = sparse.hstack([X_txt, X_meta, X_ptrn], format="csr")

            ros = RandomOverSampler(random_state=RNG)
            X_bal, y_bal = ros.fit_resample(X_type, y_type)

            bundle.type_classifier = XGBClassifier(**TYPE_PARAMS).fit(X_bal, y_bal)
        else:
            logger.warning("ìœ í˜• ë¼ë²¨ì´ ë¶€ì¡±í•´ ê³µê²© ìœ í˜• ë¶„ë¥˜ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")

        # 4-4. ì €ìž¥
        bundle.version = self.version
        bundle.meta = {
            "version": self.version,
            "trained_at": datetime.utcnow().isoformat() + "Z",
            "total_samples": len(y),
            "optimization_level": TRAINING_OPTIMIZATION_LEVEL,
        }
        bundle.save(self.out_dir)

        logger.info("í•™ìŠµ ì™„ë£Œ (%.1fs)", time.time() - t0)
        return {"status": "ok", "version": self.version}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. FastAPI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def robust_incremental_training(csv_path: str, model_version: str, **_) -> Dict[str, str]:
    return UnifiedTrainer(model_version).train(csv_path)
